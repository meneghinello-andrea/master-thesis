%---------------------------------------------------------------------------------------------------
%		deployments.tex
%
%	This is the main file of the chapter that talk about the paas layer of SPI.
%
%	Author: Andrea Meneghinello
% Version: 0.1
%	Table of changes:
%		18/03/2016 -> document definition
%---------------------------------------------------------------------------------------------------
\section{Units of deployments in the \acs{paas}}
\label{sec:background-deployments}
In the following section we analyse two different deployments units that \ac{paas} providers make
available to us to build our application or services. Both of them are based on the virtualization
but they differ at which level provide them.

\subsection{Virtualization}
\label{sec:background-deployments-virtualization}
Both \ac{paas} and \ac{iaas} layer of \ac{spi} model are provided through virtualization techniques.
Even if cloud computing can be provided without using virtualization technologies, they allow to
reduce operative and capital costs.

With the term \keyword{virtualization} we refer to the act of creating a virtual version of something,
including, but not limited, to a virtual computer hardware platform, \acs{os}es, storage devices or
network resources. Thus we can assert that virtualization is an isomorphism\footnote{Isomorphism is
a term that derive from the mathematics	domain and refers to a bijective application between objects
from different set.} between the virtual system, known as \keyword{guest}, and real one known as
\keyword{host}.

Through virtualization we want to provide high levels of performance and also gain in scalability,
reliability, availability and also in agility. Virtualization provides us an artificial view of the
physical world, on which many resources are viewed as single one or the opposite (one resource
is viewed as many individuals). Furthermore it can make a single large storage resource appear to be much
smaller or the opposite (many storage devices can appear as a big single device). It is also able to group
related processes together without making them able to see processes of other groups.

In this area virtualization provides a virtual version of a computer system, known with the name of
\keyword{\acf{vm}}. \citeauthor{vmArchitecture} in \cite{vmArchitecture} define a \ac{vm} as:

\begin{quote}
	``a component that provides a complete, persistent system environment that support an \acs{os}
	along with its many users's processes. It provides the guest operating system which access to
	virtual hardware resources including networking, I/O, \ac{gui} along with processors and memory.''
\end{quote}

Virtualizing physical infrastructures helps to improve their scalability and their utilization. It is 
important to notice that it also enables the \acs{it} personnel to perform administration tasks in an
easier way. In the next section we will do a brief digression about the \ac{iaas} model and analyse
which assets are provided the other \ac{spi} models.

\subsubsection{Assets for cloud computing}
\label{sec:background-deployments-virtualization-assets}
As we asserted in the precedent section, \ac{paas} is possible only because \ac{iaas} model exists.
To correctly support other \ac{spi} models it must provide them the following assets:

\begin{itemize}
	\item{\keyword{computing}: means executing users' tasks with the appropriate compute capacity. It is
		able to aggregate multiple computational resources to provide more power, or sharing the available
		resources in order to support tasks coming from different users. Anyway it must ensure
		\keyword{isolation}, \keyword{security} and \keyword{fast scalability};}
	\item{\keyword{storage}: means hiding physical storage devices, their location and their sharing between
		multiple end-users. Storage virtualization provides the following features: \ac{dfs},
		artificial storage volumes, arrays of storage volumes, a greater control over the available space and
		finally that different architecture can share the same storage devices;}
	\item{\keyword{networking}: means hiding the real complexity of the network by separating (or aggregating)
		the overall network system into manageable parts. Usually network virtualization provides the 
		following features: routing, \ac{nat}, isolation and aggregation.}
\end{itemize}

\subsubsection{Virtualization types}
\label{sec:background-deployments-virtualization-types}
There exists two types of virtualization and they differ for the level in which they offer it; they
are: \keyword{hardware level virtualization} (virtualize over \ac{isa}) and \keyword{\acs{os}-level
virtualization} (virtualize over \ac{abi} level). In this thesis we are mostly interested in 
\acs{os}-level virtualization; Further analysis about the first one can be found in
\citeauthor{gardimanThesis}'s master thesis \cite{gardimanThesis}. Figure
\ref{img:background-deployments-virtualization-assets} shows a generic computer architecture and
highlights the possible level of virtualization.

\begin{figure}[h!]
	\centering{}
	\includegraphics[width=0.4\textwidth]{chapters/background/images/computing-virtualization.png}
	\caption[Different computing virtualization levels]{Overview of computer architecture that shows in
		which layer we can provide virtualization. \acf{iaas} allow virtualization over \acf{isa}, instead
		\ac{paas} provide virtualization over \acf{abi} and finally \ac{saas} allows virtualization over
		\acf{api} \cite{virtualizationLevel}.}
	\label{img:background-deployments-virtualization-assets}
\end{figure}

To be thorough, we are going to spend some words about hardware level virtualization in order to correctly
understand the difference with the other type. At Hardware level the virtualization's isomorphism is managed
by a software layer called \keyword{hypervisor}, which can be of two different types (they are shown in
Figure \ref{img:background-deployments-virtualization-types}):

\begin{itemize}
	\item{\keyword{native} (type 1): a native hypervisor runs directly over the physical hardware ensuring
		a complete control of the same. In this case the hypervisor must be compatible with all the physical
		hardware components thus being able to manage it;}
	\item{\keyword{hosted} (type 2): a hosted hypervisor runs over an host \acs{os} which lies above the
		physical architecture; this topology delegates to the host \acs{os} the following tasks: hardware
		drivers management, available memory management, resources allocation management and finally process
		scheduling.}
\end{itemize}

The hosted hypervisor can produce a reduction of the perceived performance because of the presence of a 
further layer that the native type does not have. As a consequence a system, call originated by an 
application or service inside one \ac{vm}, needs more time to reach the underlying hardware downgrading 
the general performance.

\begin{figure}
	\centering{}
	\includegraphics[width=0.6\textwidth]{chapters/background/images/virtualization-types.png}
	\caption[Virtualization and containerization]{The stacks generated by the two different type of
		virtualization. We can see the native (type 1) on the left and hosted one (type 2) on the right
		\cite{virtualizationTypes}.}
	\label{img:background-deployments-virtualization-types}
\end{figure}

The hypervisor can virtualize the underlying hardware using three different approaches:

\begin{itemize}
	\item{\keyword{full-virtualization}: the hypervisor completely virtualizes the underlying hardware,
		ensuring a full compatibility with any \acs{os} that supports the physical infrastructure;}
	\item{\keyword{hardware-assisted virtualization}: the hypervisor is built as an extension of the
		previous one (full-virtualization). In this case the processor has a built-in architectural support
		that facilitate the forward phase, to the underlying hardware, of a system call generated by an
		application or service that runs inside a \ac{vm};}
	\item{\keyword{para-virtualization}: hypervisor does not offer an interface that simulates the
		underlying hardware but exposes a modified one to \ac{vm}s (closer the real one), called Virtual
		Hardware \acs{api}, explicitly designed for the \ac{vm}s; an advantage of this type of virtualization
		is that it is possible to implement an interface agnostic to the underlying hardware; instead
		the major weakness is that in case of a system crash all the current \ac{vm}s will be affected by
		the problem.}
\end{itemize}

Unlike hardware level virtualization, \acs{os}-virtualization  provides a virtual view over the
\ac{abi}. In this case the guest and the host share same \acs{os} and host hardware.

Users' processes execute on the host and they are (as much as possible) isolated from each other.
Logical separated environments are called \keyword{containers}.
Figure \ref{img:background-deployments-virtualization-containerization} shows the
difference between \acs{os} level virtualization and hardware level one.

Containers technology has existed for a long time. Examples of containers that runs outside Linux
ecosystem are Solaris Zones \cite{solarisContainers}, and \acs{bsd} jails \cite{bsdContainers}.
Instead containers' technology for the Linux ecosystem are: Linux-Vserver \cite{vserverContainers}
and OpenVZ \cite{openvzContainers}. Even though all these technologies have matured, they did not make
significant progress in order to create a proper standard technology.

The containers landscape changed when Linux containers reached a level of maturity that lead them 
into the mainstream Linux kernel with the project \keyword{\ac{lxc}}. This last one uses mainly two
kind of Linux kernel features: \keyword{namespaces} and \keyword{cgroups}.

\begin{figure}
	\centering{}
	\includegraphics[width=0.7\textwidth]{chapters/background/images/containerization.png}
	\caption[Difference from containerization and virtualization]{Difference from the generated stack
		by hardware level virtualization (type 2) and containerization.}
	\label{img:background-deployments-virtualization-containerization}
\end{figure}

The namespace is able to isolate processes resources though the following namespaces:

\begin{itemize}
	\item{PID: processes residing in a namespace cannot affect others namespace's processes. This
		means that processes inside a container are not able to see or alter processes that reside in
		another container;}
	\item{NET: each net-namespace have a different, and isolated view of the network interfaces. Each
		one has its own routing-table, iptable chains and rules;}
	\item{IPC: each process can communicate only with processes that are in the same \ac{ipc} group. This
		is necessary to maintain process isolation;}
	\item{MNT: processes that live in different mnt-namespace can see other sets of mounted \ac{fs} and
		root directories. If a \ac{fs} is mounted in a mnt-namespace, it will be accessible only to those
		that are in the same namespace;}
	\item{UTS: it is able to give containers a different host-name.}
\end{itemize}

Instead cgroups is able, through a set of rules, to measure and limit the resource consumption on which
containers have access.

\ac{lxc} is a good technology that allows users to run several applications isolated from each other. It
is able to assure an amount of resource capacity to each container. However, even with these features,
Linux containers have never reached a wide level of adoption because of their lack in handiness. Everything
has changed when a company named ``docCloud'' delivered an open-source project called \keyword{Docker}.
We will deep about Docker and its architecture in Section \ref{sec:background-deployments-docker}.

\subsubsection{\acf{kvm}}
\label{sec:background-deployments-virtualization-kvm}
\ac{kvm} is a Linux feature that allows Linux to act as a type 1\footnote{See Section
\ref{sec:background-deployments-virtualization-types} for additional information about type 1 and type 2
hypervisor.} hypervisor running an unmodified guest \acs{os} inside a Linux process. \ac{kvm} uses hardware
virtualization features in recent processors to reduce complexity and overhead.

\ac{kvm} supports both emulated \acs{io} devices (through \ac{qemu}) and paravirtual \acs{io} devices
(through virtio). The combination of hardware acceleration and paravirtual \acs{io} is designed with
the aim to reduce the virtualization overhead to very low levels \cite{mcdougall2010virtualization}.
\ac{kvm} is also able to support ``live migrations'' which permit to hardware engineers to evacuate
some or all servers inside a data-centre for maintenance without disrupting the guest \acs{os}.

Since a \ac{vm} has a static number of virtual-\acs{cpu}s and a fixed amount of \acs{ram}, its resource
consumption is bounded. A single virtual-\acs{cpu} cannot use more than one real \acs{cpu} and each page
of virtual-\acs{ram} maps to at most one page of physical pages in physical \acs{ram} (plus the nested page
table). \ac{kvm} can resize \ac{vm}s while they are running through ``hotplugging'' and ``ballooning''
of v\acs{cpu} and v\acs{ram}, although these techniques require support from the guest \acs{os} and it
is rarely used in real cloud platforms.

Because each \ac{vm} is a process, all normal Linux resource management facilities, like scheduling and
cgroupgs (see Section \ref{sec:background-deployments-virtualization-types}) apply to \ac{vm}s. This
simplifies the implementation and administration of the hypervisor but complicates resource management
inside the guest \acs{os}.

\acs{os}s generally assume that \acs{cpu}s are always running and the physical memory has relatively
fixed access times, but under \ac{kvm} v\acs{cpu}s can be unscheduled without notification and v\acs{ram} can
be swapped out, causing performance anomalies that can be hard to debug. \ac{vm}s also have two level
of allocation and scheduling: one in the hypervisor and one in the host \acs{os}. Many cloud providers
eliminate those problems by not overcommitting resources, pinning each v\acs{cpu} to a physical one,
and locking all v\acs{ram} into real \acs{ram}. These techniques essentially eliminate scheduling
activities inside the hypervisor. Moreover fixed resource allocation also simplifies the billing
phase.

\ac{vm}s naturally provide a certain level of isolation and security because of their narrow interface;
the only way a \ac{vm} can communicate with the outside world is through a limited number of hyper-calls
or emulated devices, both of which are controlled by the hypervisor. This is not a panacea, since a few
hypervisor privilege escalation vulnerabilities have been discovered and they could allow a guest \acs{os}
to ``break out'' of its \ac{vm} ``sandbox''.

While \ac{vm} excel at isolation, they add overhead when sharing data between guests or between guest
and hypervisor. Usually such sharing requires fairly expensive marshalling and hyper-calls. In the
cloud, \ac{vm}s generally access storage through emulated block devices backed by image files; creating,
updating and deploying such disk images can be time-consuming and collections of disk images with
mostly-duplicate contents can waste the available storage space.

\subsubsection{Main characteristics}
\label{sec:background-deployments-virtualization-characteristics}
Virtualization has three main characteristics: partitioning, isolation and encapsulation.

\keyword{Partitioning} refers to the fact that many applications and \acs{os}s are supported in a single 
physical system by separating (partitioning) the available resources. \keyword{Isolation} refers to the
fact that each \ac{vm} is isolated from its physical system and other \ac{vm}s; because of isolation, if
one virtual instance crashes it does not affect the others\footnote{Except in case of para-virtualization
(see Section \ref{sec:background-deployments-virtualization-types}).} and data are not shared between
different \ac{vm}s\footnote{If data sharing between \ac{vm}s is a requirement it must be implemented as
a software feature.}. Finally \keyword{encapsulation} property derives from the fact that \ac{vm}s are
represented (and even stored) as a single file, hence system administrators can identify them easily
based on the service that they provide.

From the cited properties we can affirm that cloud computing and Virtualization are not interchangeable
because they approach the \acs{it} goals from different perspectives: virtualization has the aim to
isolate computing resources in order to make possible the change of underlying layers without worrying
about higher levels; instead cloud computing has the ability to make computing resources available on
demand providing us the pay-per-use paradigm (see Section \ref{sec:background-cloudComputing}).

\subsection{Docker containers}
\label{sec:background-deployments-docker}
Docker started its life as an open-source project at “dotCloud”, a cloud-centric PaaS company, in early
2013.

Initially Docker was the natural extension of the technology that company had developed to run its cloud
business on thousands of servers. It was written in Go, a statically typed programmed language developed
by Google with syntax loosely based on C. In the months following the company, having regard to the obtained
results, joined the Linux Foundation and changed its name to Docker Inc. and announced that was shifting
its focus on the development of Docker and its ecosystem.

Docker is an open platform for developing, shipping and running applications, it is designed to deliver
software as faster as possible thanks to a shorter cycle between writing and running code. It allows the
separation of the application from the infrastructure and treats the last one like a managed application.
It allows this combining a lightweight container virtualization platform with workflows and tools
that help in managing and deploying applications.

In its core, Docker provides a way to run almost any application securely isolated in a container.
Isolation and security allow to run many containers simultaneously on the same host. Its lightweight
nature, which run without the extra load of a hypervisor, means that we can get more out from the
hardware.

Tools and a platform surround containers so they can help in several tasks:

\begin{itemize}
	\item{getting applications (and supporting components) into containers;}
	\item{distributing and shipping containers to different teams for further development and testing;}
	\item{deploying applications to production environment, whether it is in local data-centre or in the
		cloud.}
\end{itemize}

In the following sections we will discuss about its architecture, and how it is able to manage users'
data and resolve the dependency-hell problem. Finally we will do brief a digression about the state-of-art
in security.

\subsubsection{Docker architecture}
\label{sec:background-deployments-docker-architecture}
Docker is based on a client-server architecture: the \keyword{client} and \keyword{daemon}. Docker
client ``talks'' with the daemon, which does the hardest work of building, running and distributing
the containers. Users cannot directly interact with the daemon: they must write commands for it 
through the client.

Both the Docker client and the daemon can run on the same system, or it is possible to connect the
first one to a remote daemon. They can communicate via sockets or through a RESTful \acs{api}.

The Docker ecosystem, shown in Figure \ref{img:background-deployments-docker-architecture-architecture},
is composed by the following components: \keyword{images}, \keyword{registries} and finally
\keyword{containers}.

\begin{figure}
	\centering{}
	\includegraphics[width=0.8\textwidth]{chapters/background/images/docker-architecture.png}
	\caption[Docker architecture overview]{Docker architecture overview. We can see that a command
		received by the Docker client is forwarded to the Docker daemon which manages the container
		life-cycle. To the right there is the registry that holds the published images.}
	\label{img:background-deployments-docker-architecture-architecture}
\end{figure}

A Docker image is a read-only template that Docker uses to create the runnable container; it provides
a simple way to build new images or to update existing ones. A Docker image is created defining a
file (called dockerfile), that contains a description of the application/service (and its dependencies)
that will be contained when it runs. Each image consists of a series of layers; they are combined together
into a single and compact image thanks to the use of \acf{ufs}\footnote{We will discuss about \ac{ufs}
	afterwards when we will cover how data are managed.}. Docker images are the build component of the
Docker ecosystem.

Docker registries hold images; these can be public or private stores from which we can upload or
download images. The most famous public registry is provided by Docker and it is called Docker Hub, while
private ones are created and maintained by private companies for proprietary use. Registries serve a
huge collection of existing ready-to-use images. They are the distribution component of the Docker
ecosystem.

Finally Docker containers are an isolated and secure application platform, similar to a system directory
that holds everything that is needed by the application to run. When users decide to deploy a new
container, they make a request to the Docker client; the client forwards the request to the Docker
daemon, which reads the corresponding image and uses it to “give life” to the container. Docker containers
can be run, started, stopped, moved, and deleted easily. They are the run component of the Docker ecosystem.

With Docker containers we can achieve isolation and resource control (important features of SaaS)
through the use of namespaces and cgroups (see Section \ref{sec:background-deployments-virtualization-types})

To manage isolation and resource control, Docker largely uses Linux-kernel features, but different kernels
can have contrasting interfaces for the same functionalities. This is the reason why Docker, since v0.9,
replaced the old \ac{lxc} execution environment (which contains cgroups and namespaces) with
\keyword{LibContainer} (shown in Figure \ref{img:background-deployments-docker-architecture-libcontainer}).

\begin{figure}
	\centering{}
	\includegraphics[width=0.6\textwidth]{chapters/background/images/libcontainer.png}
	\caption[Docker libcontainer overview]{Now Docker execution environment is separated from the underlying
		\acs{os} though the introduction of LibContainers, which provide a standard interface with the
		underlying \acs{os}.}
	\label{img:background-deployments-docker-architecture-libcontainer}
\end{figure}

LibContainer is now the default Docker execution environment. It is meant to be a cross-system abstraction
layer being an attempt to standardize the way applications are packed up, delivered and run in isolation.
Thus, container features available in Linux kernel \acs{api} are provided as a unique library in a
consistent way; it addresses the problem of having an unique kernel \acs{api} and several implementations.

LibContainer has become a stand alone project, after Docker decided to donate it to the open-source community,
therefore other developers (Google, Parallels, RedHat, Ubuntu and others) can contribute to improve its development.

As we just stated, Docker containers are executed in isolation, but this do not prevent them to share data using
the network. Containers can be connected together in two different ways: through a connection based on 
IP-address/TCP-port or the ``\textit{link}'' feature.

In the first case containers, that have an unique IP address, must open outward a TCP port on which they can
listen for incoming connections. Even if it seems reasonable using this type of network connection to share
data, this can instead lead to security problems because connections can arrive also from the web. On the other
hand establishing a connection, between containers, through the link feature is like building a \ac{vpn};
which means less risks for the safety factor, because the connection is only available between different
containers behind the same firewall\footnote{We will analyse in Section \ref{sec:measurements-network-result}
the benefit of this feature.}.

\subsubsection{Data management}
\label{sec:background-deployments-docker-dataManagement}
Docker images are read-only templates from which Docker containers are lunched. Each image consists of
a series of layers combined into a single image using the \acf{ufs} technology.

One reason why Docker is lightweight derives from these layers. When a developer changes a Docker image,
e.g. to deploy a new version of an application, a new layer is built. Thus, rather than replacing
or entirely rebuilding the whole image, as in case of \ac{vm}s, only that layer is added or updated thus
making the Docker image distribution faster and simpler; just this one will be distributed.

\ac{ufs} (shown in Figure 6) is a stackable \ac{fs} which implements the union-mount features. That allows
to specify a series of different \ac{fs} (called branches) which are finally presented as a virtual
and coherent one, even though the branches come from different \ac{fs}. This process is commonly
referred to as namespaces unification.

\begin{figure}
	\centering{}
	\includegraphics[width=0.4\textwidth]{chapters/background/images/unionfs.png}
	\caption[Docker \ac{ufs} overview]{Example of file renaming using the \acf{ufs}. The user's process
		see only one logical file and it is responsibility of \acf{vfs} to correctly map the rename command
		to different underlying \acf{fs}.}
	\label{img:background-deployments-docker-architecture-unionfs}
\end{figure}

\ac{ufs} uses a simple priority model which gives a unique priority to each branch . If a file exists
in multiple branches, only the copy in higher priority branch will be shown. It also allows some branches
to be read-only, but as long as at least one of them is readable and writeable, \ac{ufs} uses ``copy-on-write''
semantic to provide an illusion that all branches are writeable.

To maintain \ac{fs} integrity, Docker containers are composed by read-only layers except for the 
upper one. Another key characteristic of the Docker containers is that its \ac{fs} is designed to
be ephemeral. Then if a container contains a service that write files or data when it is running 
(like a \ac{dbms} service), all of them will be lost after container termination. Thus in a future
launch of the same, into it we can only find the work environment (configured as described in 
the dockerfile) but without the previously generated files or data.

To ensure that this situation does not happens, there is the need to separate the container from
data life-cycle. Ideally we want that the generated data is not destroyed or tied to the container
life-cycle and can thus be reused. To achieve this, Docker provides the concepts of \keyword{data-volumes}
and \keyword{data-volumes container}.

A data-volume is a specially designed directory in the container that is able to “bypass” the \ac{ufs}.
It is initialized when the container is created and, by default, it is not deleted when the container
is stopped or when there is no container that reference it. Data-volumes are independently updated,
shareable across different containers and mountable in read-only mode too. This key feature is provided
through a symbolic link (also known as soft-link) that points to a \ac{fs} location in the \acs{os}
underlying docker daemon. Hence data will persist after container termination, and in a future launch
of it the only necessary thing to retrieve them is to fix the link (through a mount operation made at
container launch-time) which point at the correct location in the underlying file-system.

Instead, a data-volume container becomes useful when we want to share data between different containers
or we want to use data from non-persistent containers. They are particular containers that aim to collect
different data-volumes and make them available for other containers.

These two features are useful when we plan to provide a backup, restore or migration features in our
applications.

\subsubsection{Dependency hell problem}
\label{sec:background-deployments-docker-dependencyHell}
\citeauthor{michaeljang2006} in \cite{michaeljang2006} defines the colloquial term “dependency hell” 
to point out a set of problems. It refers to the frustration problem that software developers deal with
when installing applications with dependencies on specific versions of other software packages. Modern
applications often are assembled from existing components and delegate many tasks to third parties services
and applications, so this problem must not be under-estimated.

The problem may occur in several forms: a huge number of dependencies should be downloaded and locally saved
(large amount of downloading time and storage space may be necessary), long chain of dependencies, conflicting 
dependencies, circular dependencies and finally package manager dependencies (problem encountered when 
the packet manager is not able to download linked dependencies automatically).

Docker allows to fix, implicitly, the cited problems by packaging each component and its dependencies
into a container. In particular, with Docker containers the following issues cannot be risen:

\begin{itemize}
	\item{\keyword{conflicting dependencies}: we can run different software versions, that require
		different dependencies or many version of them, in various containers. Each one with
		the correct dependencies;}
	\item{\keyword{missing dependencies}: no dependency can be missing because every one is packaged
		along with the application container;}
	\item{\keyword{platform differencies}: moving from one provider to another is no longer a
		problem if both systems run the Docker daemon; the same container will execute without other
		issues.}
\end{itemize}

\subsubsection{Security issues}
\label{sec:background-deployments-docker-security}
Docker is becoming an interesting technology in cloud computing world and for this reason we need also
to analyse its intrinsic security issues. Speaking about Docker security, we need to focus on the following
areas: intrinsic security of Linux kernel and its support for namespaces and cgroups features, the weaknesses
of the Docker daemon itself and finally poorly configured images.

Since Docker features lie on Linux kernel, these provide an initial inherent safety. Kernel namespace
feature is the one that provides the first and straightforward form of isolation. Its main purpose is
to grant that processes inside a container cannot be seen or affected, by processes running
in other containers or in the host system. In addition each container gets its own network stack, meaning
that it cannot get a privileged access to the sockets or network interfaces of others. This means that
containers are just like physical machines connected to a common Ethernet switch, because all containers, on
a given Docker host, are placed over bridges interfaces. Cgroups are the key component of the Linux kernel
that provide resource accounting and limiting. They allow security ensuring that each container gets its fair
share of resources (memory, CPU, Disk I/O), and finally preventing a container to bring the system down due to
exhausting of computing resources. So while they do not play a role in preventing one container from accessing or
affecting data of other processes or containers, they are essential to fend off some \ac{dos} attacks, becoming
thus an interesting feature in multi-tenant environments. Both namespaces and cgroups features are available
together since Linux kernel 2.6.26 (released 8 years ago); they have been scrutinized on a large number of
production systems, so their design and their consequent implementations are pretty mature.

Running containers with Docker implies executing the Docker daemon on the host, and currently it requires
root privileges to perform its tasks. Hence only trusted system-users should be allowed to control and dialogue
with it. Docker images can request to the daemon to share directories (data-volumes) between a container
and the underlying host file-system. This means that a malicious user can start a container in which its 
``/host'' directory will be mapped to the ``/'' directory of the underlying host \acs{os}. Hence the container
will be able to alter the host file-system without restrictions while it should not have this permission.
This has a strong security implication when a cloud provider plan to provide a public \acs{api} to deploy
containers. It should be more careful than usual with the parameter checking to make sure that a malicious
user cannot pass crafted parameters causing Docker to create arbitrary containers. For this reason the Docker
REST \acs{api} endpoint is changed using, from version 0.5.2, the UNIX socket instead of a TCP bound on
127.0.0.1; This have been done because the latter are prone to \ac{csrf} attacks. Docker daemon can also be
attacked through images loading from disk or from network. This has been focus of improvements in the community,
especially for images coming from not-trusted networks. Hence from Docker 1.3.2 images are now extracted in a
chrooted sub-process on the Linux platform. In conclusion it is expected that the Docker daemon will run with
restricted privileges, delegating well-audited operations to sub-processes, each one with its own very limited
set of privileges to avoid that malicious containers can exploit vulnerabilities. This will be possible with the
future adoption of the recent improvements in Linux namespaces which includes the user-namespace. Moreover, this
will also solve the problem caused by sharing \ac{fs}s between host and guest, since the user namespace
allows users (including the root user) to be mapped to other users in the host system within containers.

Finally, many security lacks come from wrong configuration provided by the users when they define dockerfiles.
In this scenario common mistakes include software from unknown sources or wrong network configuration that
pave the way for malicious users.